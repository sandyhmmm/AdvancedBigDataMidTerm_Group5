from pyspark import SparkContext, SQLContext
import numpy as np
from sklearn import linear_model


#Load and parse the data 
sparkHome = os.environ.get('SPARK_HOME')
fileLocation = sparkHome + "/Assignment1/winequality-white.csv";


f = open(fileLocation);
f.readline();
data = np.loadtxt(fname=f,delimiter = ';');
X = data[:,0:11];
Y = data[:,11];

print(X);
print(Y);
clf = linear_model.SGDClassifier()
clf.fit(X, Y)

linear_model.SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
        eta0=0.0, fit_intercept=True, l1_ratio=0.15,
        learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
        penalty='l2', power_t=0.5, random_state=None, shuffle=True,
        verbose=0, warm_start=False)
print(clf.predict([[6.2,0.66,0.48,1.2,0.029,29,75,0.9892,3.33,0.39,12.8]]));




//////////////////////////////////////////////////////////////////////////

// Load training data in LIBSVM format.
#val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")


// Split data into training (60%) and test (40%).
val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)
val training = splits(0).cache()
val test = splits(1)

// Run training algorithm to build the model
val numIterations = 100
val model = SVMWithSGD.train(training, numIterations)

// Clear the default threshold.
model.clearThreshold()

// Compute raw scores on the test set.
val scoreAndLabels = test.map { point =>
  val score = model.predict(point.features)
  (score, point.label)
}

// Get evaluation metrics.
val metrics = new BinaryClassificationMetrics(scoreAndLabels)
val auROC = metrics.areaUnderROC()

println("Area under ROC = " + auROC)

// Save and load model
model.save(sc, "myModelPath")
val sameModel = SVMModel.load(sc, "myModelPath")



import org.apache.spark.mllib.classification.SVMModel
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.regression


sc = SparkContext(appName="Classification");
#Load and parse the data 
sparkHome = os.environ.get('SPARK_HOME')
fileLocation = sparkHome + "/Assignment1/winequality-white.csv";
sc = SparkContext(appName="Classification");
#val fileData = sc.read_csv(fileLocation);

#val splitData = fileData.getLines().map(_.split(";"));
#splitData.take(1);
#val data = splitData.map(m=> )


