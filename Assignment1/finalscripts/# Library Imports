# Library Imports

from pyspark.mllib.classification import SVMWithSGD, SVMModel

from pyspark.mllib.regression import LabeledPoint

from pyspark import SparkContext

from pyspark.mllib.feature import StandardScaler

# Load and parse the data

def parsePoint(line):

    values = [float(x) for x in line.split(',')]

    return LabeledPoint(values[11], values[0:10])



sc = SparkContext("local", "SVMWithSGD")

sparkHome = os.environ.get('SPARK_HOME')
fileLocation = sparkHome + "/Assignment1/data.csv";

data = sc.textFile(fileLocation)


parsedData = data.map(parsePoint)

#Split training/test sets

(trainingData, testData) = parsedData.randomSplit([0.6, 0.4])

#Scaling features 

label = trainingData.map(lambda x: x.label)

features = trainingData.map(lambda x: x.features)

scaler1 = StandardScaler().fit(features)

trainingData = label.zip(scaler1.transform(features))

trainingData = trainingData.map(lambda (x,y) : LabeledPoint(x,y))

# Build the model with the training for 100 iterations using regularization 

#parameter as 0.01 and regularization type as ‘l1’ using the below algorithm

model = SVMWithSGD.train(trainingData, 

iterations=100,regParam=0.01,regType='l1')

# Evaluating the model on test data

labelsAndPreds = testData.map(lambda p: (p.label, model.predict(p.features)))

#Calculating the Training error using the following formula 

trainVal = labelsAndPreds.filter(lambda (v, p): v != p).count() 

trainCount = float(testData.count())

trainErr = trainVal/trainCount

#Calculating the Accuracy using the following formula 

accuracy = labelsAndPreds.filter(lambda (v, p): v == p).count() / float(testData.count())

#Print the Training Error and Accuracy calculated

print("Training Error = " + str(trainErr))

print("Accuracy = " + str(accuracy))

sc.stop()
